# Prompt Engineering Portfolio

![Markdown](https://img.shields.io/badge/format-Markdown-blue)
![LLM Evaluation](https://img.shields.io/badge/focus-LLM%20Security%20Evaluation-orange)

This repository showcases practical examples of prompt engineering for Large Language Models (LLMs), with a focus on security, evaluation, and advanced prompting techniques.

## üéØ Portfolio Purpose

This project explores how prompts can: - Expose vulnerabilities and edge cases - Shape reasoning and output structure - Reveal model‚Äëspecific quirks and biases - Support safer, more predictable use of LLMs The emphasis is on **failure mode analysis**, **red‚Äëteaming style probing**, and **systematic refinement**, rather than one‚Äëoff ‚Äúclever prompts‚Äù.

---

## Experiments Overview

01 ‚Äî Baseline evaluation: Establishes baseline outputs for zero-shot prompts, highlighting default behaviours and initial failure points.

02 ‚Äî Few-shot prompting: Uses exemplars to shape outputs and reduce variance, revealing sensitivity to example selection and format consistency.

03 ‚Äî Chain-of-thought: Elicits explicit stepwise reasoning to evaluate logical coherence, error propagation, and reliance on intermediate steps.

04 ‚Äî Failure mode stress test: Applies adversarial and stress prompts to probe model resilience under edge-case conditions and pressure.

05 ‚Äî Failure mode analysis: Documents contradictory, impossible, ambiguous, and overloaded formatting prompts ‚Äî simulating red-team style attacks and elicitation attempts.

Each experiment includes a main analysis file and a full appendix of raw transcripts for reproducibility.

Each experiment includes: - a main analysis file - a full appendix of raw transcripts - clear goals, observations, and refinements

---

## About Me

I am a detail‚Äëoriented technician, digital product specialist, and mythic creative writer based in Dublin. My background spans IT troubleshooting, enterprise patching workflows, and front‚Äëend development, combined with formal training in OSINT and AI (Google AI Essentials, DeepLearning.AI Prompt Engineering). I fuse hands‚Äëon technical realism with layered storytelling, building reproducible prompt engineering experiments that surface vulnerabilities and illuminate failure modes.

This portfolio reflects my commitment to **authenticity**, **clarity**, and **reproducibility**. I thrive on designing prompts that expose edge cases, documenting outputs with precision, and synthesising technical and symbolic meaning.

### Notes

Location: Dublin, Ireland
Date of completion: December 2025
Repository: [Prompt Engineering Portfolio](https://github.com/Hermeticpoet/Prompt_Engineering_Portfolio)

## Models Evaluated

The portfolio includes experiments with:

- **Grok 4 (default chat via grok.com)**
- **Microsoft Copilot (Windows & web integrations)**
- **Gemini (default tier)**

Each entry includes:

- **Goal**: What the prompt was designed to test
- **Prompt(s)**: The exact text used
- **Model Output**: Representative response
- **Analysis**: Observed strengths, weaknesses, or vulnerabilities
- **Refinement**: Adjustments made to improve results

## Categories

1. Security & Red‚ÄëTeaming (jailbreaks, injections, data elicitation)
2. Advanced Prompting (few‚Äëshot, chain‚Äëof‚Äëthought, role‚Äëplaying)
3. Evaluation & Refinement (failure mode analysis, structured outputs)

## How to Use

Browse the `examples/` folder for individual case studies. Each file is self‚Äëcontained and can be read independently.

## Portfolio examples

- [01 ‚Äî Jailbreak resistance test](examples/01_jailbreak_test.md)  
  ‚Ü≥ [Appendix 01 ‚Äî Full Output](appendices/01_jailbreak_full.md)

- [02 ‚Äî Few-shot prompting](examples/02_few_shot_prompting.md)  
  ‚Ü≥ [Appendix 02 ‚Äî Full Output](appendices/02_few_shot_full.md)

- [03 ‚Äî Chain-of-thought evaluation](examples/03_chain_of_thought.md)  
  ‚Ü≥ [Appendix 03 ‚Äî Full Output](appendices/03_chain_of_thought_full.md)

- [04 ‚Äî Structured output (JSON)](examples/04_structured_output.md)  
  ‚Ü≥ [Appendix 04 ‚Äî Full Output](appendices/04_structured_output_full.md)

- [05 ‚Äî Failure mode analysis](examples/05_failure_mode_analysis.md)  
  ‚Ü≥ [Appendix 05 ‚Äî Full Output](appendices/05_failure_mode_analysis_full.md)

## Repo structure & reproducibility

- `examples/` ‚Üí Concise case studies (goal, prompt, output snippet, analysis, refinement).
- `appendices/` ‚Üí Full transcripts of model runs for transparency.

Each case study includes:

- Prompt(s) and outputs
- Analysis and refinement
- Notes on reproducibility (parameters, date, environment, model card link)
